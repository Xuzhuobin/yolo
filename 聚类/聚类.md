# 第一章 聚类理论基础

## 1.1 聚类任务

聚类：通过算法将样本聚集成多个不同的簇，根据聚类的结果将每个簇定义成一个类，后续根据类去训练分类模型。



## 1.2 性能度量-衡量聚类的有效方法

基本思想：簇内相似度高，不同簇的样本尽可能不同。

### 1.2.1 外部指标

1.定义：将聚类结果和某个“参考模型”进行对比。

2.Jackcard系数、FM指数、Rand指数

### 1.2.2 内部指标

1.定义：直接考察聚类结果

2.DB指数、Dunn指数

## 1.3 距离计算

1.定义：用于计算样本间的距离，同一类簇中的样本距离尽可能近，不同类簇中的样本距离尽可能远。

### 1.3.1 有序属性距离

1.定义：样本点存在“**数值属性**”，比如说{1，2，3，4，5}。

2.常用方法：

![image-20220605204155582](C:\Users\DELL\Desktop\YOLO笔记\聚类\聚类.assets\image-20220605204155582.png)

### 1.3.2 无序属性距离

1.定义：样本点存在"**列名属性**"，比如说{红、绿、黄、紫}。

2.VDM距离：

![image-20220605205043582](C:\Users\DELL\Desktop\YOLO笔记\聚类\聚类.assets\image-20220605205043582.png)

说明：

- m(u,a,i)表示在第i个样本簇中，特征u取值为a的样本数；

- 分数形式是为了将不同特征的距离都统一到(0,1)之间；

- 本质上是计算所有样本簇中某一个特征不同取值的距离；

  

### 1.3.3 混合属性

1.定义：样本属性中有nc个有序属性、(n - nc)个无序属性。

2公式：

![image-20220605205637281](C:\Users\DELL\Desktop\YOLO笔记\聚类\聚类.assets\image-20220605205637281.png)





# 第二章 聚类方法

## 2.1 kmeans

1.过程：

- 随机选取k个样本作为簇心；
- 将其他所有样本按照和簇心的距离分配类别；
- 根据划分好的簇计算新的簇心，更新原来的簇心；
- 重复2~3直到簇心不再更新；

**说明：簇心的计算使用簇中所有样本的均值向量。**

存在问题：初始点的选择直接影响聚类的结果，随机性较大；

​					对特殊的样本分布情况，聚类的效果差；



## 2.2 学习向量量化

1.说明：学习向量量化方法适用于样本数据带有**类别标记**的情况。(带约束条件的半监督聚类？？？)

2.过程：

- 随机选择k个类别标记为ti(i = 1,2...k)的样本作为初始化原型向量，将他们的类别标记作为聚类簇的簇标记；
- 从剩下的样本中选取一个**样本**，找到与它距离最近的**原型向量**，比较类别标记和簇标记是否一致。若一致，原型向量向样本点靠近；反之则远离；
- 重复2直到原型向量不在更新或达到最大迭代轮数；



## 2.3 高斯混合聚类

### 2.3.1 背景知识

1.高斯混合分布：

​	![image-20220606145607665](C:\Users\DELL\Desktop\YOLO笔记\聚类\聚类.assets\image-20220606145607665.png)

说明：

- 样本的生成由多个满足高斯分布的**成分**组成，每一个成分占比为混合系数a；
- 可以理解为每个成分即为一个簇，每个簇在总样本中的占比为a;
- 对于某一个样本，只需要找到它在由第i个成分生成的概率，即可找到它所在的簇；



### 2.3.2 EM算法



​			

## 2.4 密度聚类(DBSCAN)

1.过程：

- 遍历所有样本，将**邻域e**内样本数**大于阈值**的点设置为核心对象；
- 随机从一个核心对象出发，将所有密度可达且为被访问的点放入一个簇内；
- 重复2直到所有样本点被访问；

说明：只需要设置**邻域距离**、**点数阈值**两个参数；

​			簇个数由算法自动确定；



## 2.5 层次聚类(AGNES)

1.过程：

- 将每一个样本作为一个类簇；
- 将**距离**较小的类簇进行合并；
- 重复2直到达到预设的类簇个数；

2.距离：

![image-20220606110819322](C:\Users\DELL\Desktop\YOLO笔记\聚类\聚类.assets\image-20220606110819322.png)



# 第三章 谱聚类

https://www.cnblogs.com/pinard/p/6221564.html

## 3.1 背景知识

### 3.1.1 无向权重图

1.权重矩阵：

​		对于一个图G，我们一般用点的集合V和边的集合E来描述。即为G(V,E)。其中V即为我们数据集里面所有的点(v1,v2,...vn)。对于V中的任意两个点，可以有边连接，也可以没有边连接。我们定义权重wij为点vi和点vj之间的权重。由于我们是无向图，所以wij=wji。由此就可以得到一个矩阵用来描述所有样本之间的关系，即为图的邻接权重矩阵W。

2.度：

​		对于图中的任意一个点vi，它的度di定义为和它相连的所有边的权重之和，即：
$$
d_{i}=\sum_{j=1}^{n} w_{i j}
$$
​		利用每个点度的定义，我们可以得到一个nxn的度矩阵DD,它是一个对角矩阵，只有主对角线有值，对应第i行的第i个点的度数，定义如下：
$$
\mathbf{D}=\left(\begin{array}{ccc}
d_{1} & \ldots & \cdots \\
\cdots & d_{2} & \cdots \\
\vdots & \vdots & \ddots \\
\ldots & \ldots & d_{n}
\end{array}\right)
$$
3.对于点集VV的的一个子集A⊂VA⊂V，我们定义：

​											|A|:=子集A中点的个数
$$
\operatorname{vol}(A):=\sum_{i \in A} d_{i}
$$

### 3.1.2 相似矩阵

1.相似矩阵：Sij表示i、j两个样本之间的距离。

2.样本点矩阵 ---> (利用样本点之间的距离，构建相似矩阵S) ----> (利用相似矩阵S,构建邻接矩阵W，原理是权重近小远大)

3.构建W的常见方法：

构建邻接矩阵WW的方法有三类：ϵ-邻近法，K邻近法和全连接法。



ϵ-邻近法：

　　对于ϵ-邻近法，它设置了一个距离阈值ϵ，然后用欧式距离sij度量任意两点xi和xj的距离。即相似矩阵的sij=||xi−xj||2, 然后根据sijsij和ϵ的大小关系，来定义邻接矩阵W如下：
$$
w_{i j}= \begin{cases}0 & s_{i j}>\epsilon \\ \epsilon & s_{i j} \leq \epsilon\end{cases}
$$
　　从上式可见，两点间的权重要不就是ϵ,要不就是0，没有其他的信息了。距离远近度量很不精确，因此在实际应用中，我们很少使用ϵ-邻近法。

　

K临近法：　

​		K邻近法，利用KNN算法遍历所有的样本点，取每个样本最近的k个点作为近邻，只有和样本距离最近的k个点之间的wij>0。但是这种方法会造成重构之后的邻接矩阵W非对称，我们后面的算法需要对称邻接矩阵。为了解决这种问题，一般采取下面两种方法之一：

　　第一种K邻近法是只要一个点在另一个点的K近邻中，则保留sij：
$$
w_{i j}=w_{j i}= \begin{cases}0 & x_{i} \notin K N N\left(x_{j}\right) \text { and } x_{j} \notin K N N\left(x_{i}\right) \\ \exp \left(-\frac{\left\|x_{i}-x_{j}\right\|_{2}^{2}}{2 \sigma^{2}}\right) & x_{i} \in K N N\left(x_{j}\right) \text { or } x_{j} \in K N N\left(x_{i}\right)\end{cases}
$$
　　

​		第二种K邻近法是必须两个点互为K近邻中，才能保留sij：
$$
w_{i j}=w_{j i}= \begin{cases}0 & x_{i} \notin K N N\left(x_{j}\right) \text { or } x_{j} \notin K N N\left(x_{i}\right) \\ \exp \left(-\frac{\left\|x_{i}-x_{j}\right\|_{2}^{2}}{2 \sigma^{2}}\right) & x_{i} \in K N N\left(x_{j}\right) \text { and } x_{j} \in K N N\left(x_{i}\right)\end{cases}
$$


全连接法：

　　相比前两种方法，第三种方法所有的点之间的权重值都大于0，因此称之为全连接法。可以选择不同的核函数来定义边权重，常用的有多项式核函数，高斯核函数和Sigmoid核函数。最常用的是高斯核函数RBF，此时相似矩阵和邻接矩阵相同：
$$
w_{i j}=s_{i j}=\exp \left(-\frac{\left\|x_{i}-x_{j}\right\|_{2}^{2}}{2 \sigma^{2}}\right)
$$
　　在实际的应用中，使用第三种全连接法来建立邻接矩阵是最普遍的，而在全连接法中使用高斯径向核RBF是最普遍的。



### 3.1.3 拉普拉斯矩阵

1.定义：L = D - W

2.性质：

1）拉普拉斯矩阵是对称矩阵，这可以由D和W都是对称矩阵而得。

2）由于拉普拉斯矩阵是对称矩阵，则它的所有的特征值都是实数。

3）对于任意的向量ff,我们有：
$$
f^{T} L f=\frac{1}{2} \sum_{i, j=1}^{n} w_{i j}\left(f_{i}-f_{j}\right)^{2}
$$
推导：
$$
\begin{gathered}
f^{T} L f=f^{T} D f-f^{T} W f=\sum_{i=1}^{n} d_{i} f_{i}^{2}-\sum_{i, j=1}^{n} w_{i j} f_{i} f_{j} \\
=\frac{1}{2}\left(\sum_{i=1}^{n} d_{i} f_{i}^{2}-2 \sum_{i, j=1}^{n} w_{i j} f_{i} f_{j}+\sum_{j=1}^{n} d_{j} f_{j}^{2}\right)=\frac{1}{2} \sum_{i, j=1}^{n} w_{i j}\left(f_{i}-f_{j}\right)^{2}
\end{gathered}
$$


### 3.1.2 无向图切图

1.定义：

​		对于无向图GG的切图，我们的目标是将图G(V,E)G(V,E)切成相互没有连接的k个子图，每个子图点的集合为：A1,A2,..Ak，它们满足Ai∩Aj=∅,且A1∪A2∪...∪Ak=V。

　　对于任意两个子图点的集合A,B⊂V, A∩B=∅, 我们定义A和B之间的切图权重为：
$$
W(A, B)=\sum_{i \in A, j \in B} w_{i j}
$$
**说明：两个子图之间的权重就是，两个子图之间所有向连的样本点的权重之和。**

​		那么对于我们k个子图点的集合：A1,A2,..Ak，我们定义切图cut为：
$$
\operatorname{cut}\left(A_{1}, A_{2}, \ldots A_{k}\right)=\frac{1}{2} \sum_{i=1}^{k} W\left(A_{i}, \bar{A}_{i}\right)
$$
其中A¯i为Ai的补集，意为除Ai子集外其他V的子集的并集。

2.Ncut切图：
$$
\operatorname{NCut}\left(A_{1}, A_{2}, \ldots A_{k}\right)=\frac{1}{2} \sum_{i=1}^{k} \frac{W\left(A_{i}, \bar{A}_{i}\right)}{\operatorname{vol}\left(A_{i}\right)}
$$
3.指示向量：

hj∈{h1,h2,..hk}j=1,2,...k,对于任意一个向量hj, 它是一个n维向量（n为样本数），我们定义hij为：
$$
h_{i j}= \begin{cases}0 & v_{i} \notin A_{j} \\ \frac{1}{\sqrt{\left|A_{j}\right|}} & v_{i} \in A_{j}\end{cases}
$$
**说明：hij不为0表示第i个样本属于类别j。**





## 3.2 切图聚类

1.定义：通过切图，使得所有子图之间的权重最小化。

​	目标：求得一个指示矩阵H( Y ’ )使得：argminNcut(V)。

![image-20220610161007721](C:\Users\DELL\Desktop\YOLO笔记\聚类\聚类.assets\image-20220610161007721.png)

![image-20220610161309580](C:\Users\DELL\Desktop\YOLO笔记\聚类\聚类.assets\image-20220610161309580.png)



## 3.3 谱聚类算法流程

　输入：样本集D=(x1,x2,...,xn)，相似矩阵的生成方式, 降维后的维度k1, 聚类方法，聚类后的维度k2

　　　　输出： 簇划分C(c1,c2,...ck2).　

　　　　1) 根据输入的相似矩阵的生成方式构建样本的相似矩阵S

　　　　2）根据相似矩阵S构建邻接矩阵W，构建度矩阵D

　　　　3）计算出拉普拉斯矩阵L

　　　　4）构建标准化后的拉普拉斯矩阵D−1/2LD−1/2

　　　　5）计算D−1/2LD−1/2最小的k1个特征值所各自对应的特征向量f

　　　　6) 将各自对应的特征向量ff组成的矩阵按行标准化，最终组成n×k1维的特征矩阵F

　　　　7）对F中的每一行作为一个k1维的样本，共n个样本，用输入的聚类方法进行聚类，聚类维数为k2。

　　　　8）得到簇划分C(c1,c2,...ck2)